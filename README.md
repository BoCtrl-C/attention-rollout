# Attention Rollout

The code contained in this repository stands as a didactic example presented at the Artificial Intelligence and Deep Learning course organized by Sezione Fisica Medica of Universit√† degli Studi di Roma Tor Vergata.

This repo contains the following files:
* `vision_transformer.py`, which is a modified version of [vision_transformer.py](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py) that makes it possible to recover attention matrices.
* `attention_rollout.py`, which provides the functions to compute and display attention rollout.
* `example.ipynb`, which shows how to compute and display attention rollout.
